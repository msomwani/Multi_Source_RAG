# Multi-Source RAG ğŸš€

A **production-oriented Multi-Source Retrieval Augmented Generation (RAG)** system that ingests knowledge from documents and websites, then enables conversational querying with **hybrid retrieval**, **streaming responses**, **metadata-aware citations**, and a modern UI.

This project emphasizes **evaluation-driven architecture decisions** rather than feature bloat.


## ğŸ“¸ Screenshots

### ğŸ—ï¸ Architecture (Ingestion)
![Architecture 1](docs/architecture/Ingestion.png)

### ğŸ—ï¸ Architecture (Query Pipeline)
![Architecture 2](docs/architecture/Query-pipeline.png)

### ğŸ’» UI (Light/Dark + Draft Chat)
![UI 1](docs/screenshot/new_chat_ui.png)

### ğŸ’» UI (Chat + Sources + Ingestion)
![UI 2](docs/screenshot/UI_2.png)


## âœ¨ Key Capabilities

### âœ… Multi-Source Ingestion
- File ingestion: **PDF, DOCX, TXT**
- Web URL ingestion
- OCR-supported ingestion for scanned documents
- Chunking, embedding, and storage in a vector database with metadata

### âœ… Hybrid Retrieval (Primary Pipeline)
- Combines **dense embeddings + keyword/BM25-style retrieval**
- Metadata-aware chunk retrieval with stable source attribution
- Optimized for smallâ€“medium corpora using empirical evaluation results

### âœ… Retrieval Evaluation Framework
- Custom evaluation harness to benchmark:
  - Dense vs Hybrid vs Multi-query vs Reranking pipelines
  - Latency and keyword-level correctness
- JSON-based result logging with timestamps
- Evaluation results used to **simplify and harden the production pipeline**

### âœ… Conversational Memory
- Multi-conversation support
- Chat history persisted in **PostgreSQL**
- Draft chat mode (UI-only) until first interaction

### âœ… Streaming Answers + Citations
- Token-level streaming responses
- Stable, numbered source citations shown with each assistant message
- Metadata preserved end-to-end from ingestion to answer

### âœ… Modern UI
- Clean chat interface with draft conversations
- Light/Dark mode with persistent theme
- Responsive sidebar for conversations and ingestion


## ğŸ§± Tech Stack

### Backend
- FastAPI
- PostgreSQL (conversation + message storage)
- ChromaDB (vector database)
- OpenAI API (LLM + embeddings)
- HuggingFace Cross-Encoder (evaluation & reranking experiments)

### Frontend
- React (Vite)
- Streaming via Fetch + ReadableStream
- Minimal, UX-focused chat interface


## âš¡ Quick Start (Local)

### Backend
```bash
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload
```

API Docs:
```
http://127.0.0.1:8000/docs
```

### Frontend
```bash
cd rag-ui
npm install
npm run dev
```

UI:
```
http://127.0.0.1:5173
```


## ğŸ³ Run with Docker

```bash
docker compose up --build
```

Stop:
```bash
docker compose down
```


## ğŸ”— Core API Routes

### Chat
- `POST /query` â€” Non-streaming response
- `POST /query/stream` â€” Streaming response

### Conversations
- `POST /conversations`
- `GET /conversations`
- `GET /conversations/{id}`
- `DELETE /conversations/{id}`

### Ingestion
- `POST /ingest`
- `POST /ingest/url`


## ğŸ§  Architectural Notes

- Hybrid retrieval is the **default production pipeline**, chosen after empirical evaluation.
- Multi-query expansion and reranking are retained as experimental modules but excluded from the hot path due to high latency with minimal gains.
- The system prioritizes **clarity, observability, and correctness** over feature overload.

